# SYLANA VESSEL - QUANTIZED MODEL SETUP GUIDE

**Perfect for ancient laptops and limited disk space!**

---

## WHY QUANTIZED?

### The Problem
- Full Llama 2 7B: **13.5GB** download
- Your laptop: **393MB** free space üòÖ

### The Solution
- Quantized Llama 2 7B: **~6GB** download
- Runs efficiently on **CPU-only** (no GPU needed)
- **Same features, same quality** responses
- **Faster inference** than full model on CPU

---

## QUICK START (EASIEST)

### Windows
```bash
quickstart_quantized.bat
```

### What It Does
1. Sets up `.env.quantized` configuration
2. Checks/installs dependencies
3. Offers to download the quantized model (~6GB)
4. Launches Sylana with full features

**That's it!** The script handles everything.

---

## MANUAL SETUP

### Step 1: Install Dependencies

```bash
pip install llama-cpp-python
pip install -r requirements.txt
```

### Step 2: Configure Environment

```bash
# Use the quantized configuration
copy .env.quantized .env

# Edit .env and verify your HF_TOKEN
notepad .env
```

### Step 3: Download Quantized Model

**Option A: Automatic (Recommended)**
```bash
python -c "from core.quantized_model import download_quantized_model; download_quantized_model()"
```

**Option B: Manual Download**
1. Go to: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF
2. Download: `llama-2-7b-chat.Q4_K_M.gguf` (~6GB)
3. Save to: `models/llama-2-7b-chat.Q4_K_M.gguf`

### Step 4: Initialize Database

```bash
python memory/init_database.py
```

### Step 5: Run Sylana

```bash
python sylana_quantized.py
```

---

## WHAT'S DIFFERENT?

### Same Features ‚úÖ
- ‚úÖ Full semantic memory with FAISS
- ‚úÖ Emotion detection
- ‚úÖ Advanced prompt engineering
- ‚úÖ Feedback collection
- ‚úÖ Memory explorer
- ‚úÖ All entry points work

### Different Under the Hood
- **Model Format:** .gguf (quantized) instead of .safetensors
- **Inference Engine:** llama.cpp instead of HuggingFace
- **Precision:** 4-bit quantization (Q4_K_M)
- **Hardware:** CPU-optimized (no GPU needed)

### Benefits
- **Smaller:** 6GB vs 13.5GB
- **Faster on CPU:** 2-3x faster inference on CPU
- **Lower Memory:** Uses less RAM
- **Ancient Laptop Friendly:** Works on old hardware

---

## DISK SPACE REQUIREMENTS

### Before (Full Model)
```
Llama 2 7B:           13.5 GB
DistilBERT:            0.3 GB
FAISS embeddings:      0.5 GB
Database + logs:       1.0 GB
-----------------------------------
Total:                ~15.5 GB
```

### After (Quantized)
```
Llama 2 7B Q4_K_M:     6.0 GB
DistilBERT:            0.3 GB
FAISS embeddings:      0.5 GB
Database + logs:       1.0 GB
-----------------------------------
Total:                 ~8.0 GB
```

**Savings: 7.5GB!**

---

## PERFORMANCE COMPARISON

### Full Model (HuggingFace)
- **GPU:** Fast (2-4 seconds/response)
- **CPU:** Very slow (30-60 seconds/response)
- **RAM:** 16GB+ recommended

### Quantized Model (llama.cpp)
- **CPU:** Fast (5-10 seconds/response)
- **RAM:** 8GB sufficient
- **Quality:** 95% of full model quality

**For CPU-only systems: Quantized is WAY better**

---

## MODEL VARIANTS

TheBloke provides several quantized versions. We use **Q4_K_M** (recommended):

| Variant | Size | Quality | Speed | Recommendation |
|---------|------|---------|-------|----------------|
| Q2_K    | 3GB  | 60%     | Fast  | Too low quality |
| Q3_K_M  | 4GB  | 75%     | Fast  | Acceptable |
| **Q4_K_M** | **6GB**  | **95%**     | **Good**  | **BEST BALANCE** ‚≠ê |
| Q5_K_M  | 7GB  | 98%     | Slower| Slight improvement |
| Q6_K    | 8GB  | 99%     | Slow  | Minimal gain |

**We chose Q4_K_M** because:
- Best quality/size ratio
- Still fits on your laptop
- Excellent performance

---

## TROUBLESHOOTING

### "llama_cpp not found"
**Fix:**
```bash
pip install llama-cpp-python
```

### "Model file not found"
**Fix:** Download the model:
```bash
python -c "from core.quantized_model import download_quantized_model; download_quantized_model()"
```

Or download manually from:
https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF

### Slow download speeds
**Fix:** The model is 6GB, so it may take time depending on your internet:
- Fast internet (100 Mbps): ~10 minutes
- Medium (25 Mbps): ~30 minutes
- Slow (5 Mbps): ~2-3 hours

Be patient! It's worth the wait.

### Out of memory error
**Fix:** The quantized model needs ~8GB RAM. If you have less:
1. Close other applications
2. Reduce context in `.env`:
   ```
   QUANTIZED_N_CTX=1024
   SEMANTIC_SEARCH_K=3
   ```

### Still too slow
**Fix:** Adjust thread count in `.env`:
```bash
# Auto-detect (default)
QUANTIZED_N_THREADS=auto

# Or specify (e.g., 4 threads)
QUANTIZED_N_THREADS=4
```

---

## COMPARISON: QUANTIZED VS LIGHTWEIGHT

### Quantized Model (This Guide)
- **Model:** Llama 2 7B Q4_K_M
- **Size:** 6GB
- **Quality:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (95% of full model)
- **Responses:** Natural, sophisticated
- **Setup:** Medium (download required)

### Lightweight Model (.env.lightweight)
- **Model:** DistilGPT-2
- **Size:** 500MB
- **Quality:** ‚≠ê‚≠ê‚≠ê (60% of full model)
- **Responses:** Simple, shorter
- **Setup:** Easy (instant download)

**Recommendation:**
- **If you can spare 6GB:** Use quantized (much better)
- **If desperate for space:** Use lightweight (acceptable)

---

## CONFIGURATION OPTIONS

### In `.env.quantized`:

```bash
# Model file location
QUANTIZED_MODEL_PATH=./models/llama-2-7b-chat.Q4_K_M.gguf

# Context window (how much text the model remembers)
QUANTIZED_N_CTX=2048  # Default, good balance
# QUANTIZED_N_CTX=1024  # Use less RAM
# QUANTIZED_N_CTX=4096  # Better memory, more RAM

# CPU threads (auto-detect recommended)
QUANTIZED_N_THREADS=auto
# QUANTIZED_N_THREADS=4  # Or specify manually

# Generation quality
TEMPERATURE=0.9  # Higher = more creative (0.0-2.0)
MAX_NEW_TOKENS=150  # Max response length
```

---

## ENTRY POINTS COMPARISON

After setup, you can use any entry point:

### 1. `python sylana_quantized.py` ‚≠ê (RECOMMENDED)
- **Optimized for quantized model**
- All features (semantic memory, feedback, etc.)
- Automatic model download prompt
- Best experience for quantized setup

### 2. `python sylana.py`
- Would try to use full HuggingFace model
- **Don't use this** with quantized config

### 3. `python sylana_enhanced.py`
- Semantic memory focus
- Works but use sylana_quantized.py instead

### 4. `python main.py`
- Simple interface
- Would try to use full model

**Just use `sylana_quantized.py`** - it's purpose-built for this!

---

## FAQ

### Q: Is quantized model quality good enough?
**A:** Yes! Q4_K_M retains 95% of the full model's quality. You won't notice much difference in conversations.

### Q: Do I still get semantic memory?
**A:** Yes! All features work exactly the same. Only the underlying model inference is different.

### Q: Can I use GPU with quantized?
**A:** Yes, but llama.cpp is already fast on CPU. GPU support is available if you want:
```python
# In core/quantized_model.py, change:
n_gpu_layers=0  # to n_gpu_layers=35 (or more)
```

### Q: Can I fine-tune the quantized model?
**A:** No, quantized models can't be fine-tuned. But the feedback system still works for collecting training data.

### Q: What if I get more disk space later?
**A:** Switch back to full model:
```bash
copy .env.backup .env  # Or .env.template
quickstart.bat
```

### Q: How much faster is it really?
**A:** On CPU-only systems:
- Full model: 30-60 seconds/response
- Quantized: 5-10 seconds/response
- **3-6x faster!**

---

## SUMMARY

**For your ancient laptop with limited space:**

### Quick Start
```bash
quickstart_quantized.bat
```

### What You Get
- ‚úÖ Same features as full Sylana
- ‚úÖ Only 6GB instead of 13.5GB
- ‚úÖ Runs great on CPU (no GPU needed)
- ‚úÖ Fast responses (5-10 seconds)
- ‚úÖ 95% quality of full model

### Next Steps
1. Run `quickstart_quantized.bat`
2. Let it download the model (~6GB, one-time)
3. Start chatting with Sylana!

**You're going to love it!** üöÄ

---

**Created by:** Elias Ritt with Claude
**Optimized for:** Ancient laptops everywhere
