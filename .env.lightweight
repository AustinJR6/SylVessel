# Sylana Vessel - LIGHTWEIGHT Configuration
# Use this if you have limited disk space (<15GB free)
# NEVER commit this file to git - it contains secrets!

# HuggingFace API Token
HF_TOKEN=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

# Database Configuration
SYLANA_DB_PATH=./data/sylana_memory.db

# Model Configuration - LIGHTWEIGHT MODELS
# DistilGPT-2: ~500MB (vs Llama 2 7B: ~13GB)
MODEL_NAME=distilgpt2
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Fine-Tuning Controls
ENABLE_FINE_TUNING=false
MIN_TRAINING_SAMPLES=100
CHECKPOINT_DIR=./data/checkpoints

# Generation Parameters - Optimized for smaller model
TEMPERATURE=0.9
TOP_P=0.9
MAX_NEW_TOKENS=100
MAX_CONTEXT_LENGTH=256

# Memory Configuration - Reduced for performance
MEMORY_CONTEXT_LIMIT=3
SEMANTIC_SEARCH_K=3
SIMILARITY_THRESHOLD=0.7

# Voice Configuration
ENABLE_VOICE=false
TTS_RATE=160
TTS_VOLUME=1.0

# Logging
LOG_LEVEL=INFO
LOG_FILE=./data/sylana.log
